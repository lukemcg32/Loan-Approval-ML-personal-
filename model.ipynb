{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sklearn as sk\n",
    "import torch as tt\n",
    "import torchvision as ttv\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/1037507239.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Approval'].replace(['Approved', 'Rejected'], [1,0], inplace=True)\n",
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/1037507239.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Approval'].replace(['Approved', 'Rejected'], [1,0], inplace=True)\n",
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/1037507239.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Employment_Status'].replace(['employed', 'unemployed'], [1,0], inplace=True)\n",
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/1037507239.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Employment_Status'].replace(['employed', 'unemployed'], [1,0], inplace=True)\n",
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/1037507239.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['Text'].apply(lambda x: finalpreprocess(x))\n"
     ]
    }
   ],
   "source": [
    "# apply same logic as in analysis\n",
    "\n",
    "df = pd.read_csv(\"loan_data.csv\")\n",
    "df['Approval'].replace(['Approved', 'Rejected'], [1,0], inplace=True)\n",
    "df['Employment_Status'].replace(['employed', 'unemployed'], [1,0], inplace=True)\n",
    "\n",
    "data = df[(df['DTI_Ratio'] >= 0) & \n",
    "            (df['Credit_Score'] >= 550) & \n",
    "            (df['Loan_Amount'] < 120000)]\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text = text.strip()\n",
    "    text = re.compile(r'[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    return text\n",
    "\n",
    "def stopword(string):\n",
    "    a = [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) \n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for tag in word_pos_tags] \n",
    "    return \" \".join(a)\n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "\n",
    "data['clean_text'] = data['Text'].apply(lambda x: finalpreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = data['clean_text']\n",
    "y = data['Approval']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# convert to numbers and only take 500 most impportant words ** possibly increase/decrease **\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray() # array to be handles by tensor/torch\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray() # array to be handles by tensor/torch\n",
    "\n",
    "# Numeric features\n",
    "numeric_cols = ['Income', 'Credit_Score', 'Loan_Amount', 'DTI_Ratio', 'Employment_Status']\n",
    "X_train_num = data.loc[X_train.index][numeric_cols]\n",
    "X_test_num = data.loc[X_test.index][numeric_cols]\n",
    "\n",
    "# 0 mean and unit variance for input into NN\n",
    "scaler = StandardScaler()\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "X_test_num_scaled = scaler.transform(X_test_num)\n",
    "\n",
    "# now the model is more accuracte with the income included\n",
    "# scaler_reduced = StandardScaler()\n",
    "# X_train_num_scaled_reduced = scaler_reduced.fit_transform(X_train_num.drop(['Income'], axis=1))\n",
    "# X_num_test_scaled_reduced = scaler_reduced.transform(X_test_num.drop(['Income'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_num_scaled_reduced = scaler.fit_transform(X_train_num.drop(['Loan_Amount'], axis=1))# \n",
    "# ^^ use to test without loan_amount ^^\n",
    "\n",
    "class CombinedTorchModel(nn.Module):\n",
    "    # initialize 3 layers\n",
    "    def __init__(self, text_dim, num_dim):\n",
    "        super().__init__()\n",
    "        self.text_layer = nn.Linear(text_dim, 64)\n",
    "        self.num_layer = nn.Linear(num_dim, 32)\n",
    "        self.combined_layer = nn.Linear(96, 1)\n",
    "\n",
    "    # pass into model\n",
    "    def forward(self, text_input, num_input):\n",
    "        x_text = torch.sigmoid(self.text_layer(text_input)) # more accurate with sigmoid over relu - tanh wasn't as good\n",
    "        x_num = torch.relu(self.num_layer(num_input))\n",
    "        x = torch.cat((x_text, x_num), dim=1) # concatenates the outputs from the two branches\n",
    "        return torch.sigmoid(self.combined_layer(x)) # output between 0 and 1\n",
    "    \n",
    "X_text_train_torch = torch.tensor(X_train_tfidf, dtype=torch.float32)\n",
    "X_num_train_torch = torch.tensor(X_train_num_scaled, dtype=torch.float32)\n",
    "# use unsqueeze(1) to match what model expects\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1) \n",
    "\n",
    "X_text_test_torch = torch.tensor(X_test_tfidf, dtype=torch.float32)\n",
    "X_num_test_torch = torch.tensor(X_test_num_scaled, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6544, Val Loss: 0.1991\n",
      "Epoch 2, Train Loss: 0.0358, Val Loss: 0.1240\n",
      "Epoch 3, Train Loss: 0.0289, Val Loss: 0.0996\n",
      "Epoch 4, Train Loss: 0.0448, Val Loss: 0.0840\n",
      "Epoch 5, Train Loss: 0.2017, Val Loss: 0.0755\n",
      "Epoch 6, Train Loss: 0.0075, Val Loss: 0.0680\n",
      "Epoch 7, Train Loss: 0.0000, Val Loss: 0.0623\n",
      "Epoch 8, Train Loss: 0.2273, Val Loss: 0.0571\n",
      "Epoch 9, Train Loss: 0.0409, Val Loss: 0.0539\n",
      "Epoch 10, Train Loss: 0.0000, Val Loss: 0.0495\n",
      "\n",
      "PyTorch Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1701\n",
      "           1       0.99      0.98      0.98       820\n",
      "\n",
      "    accuracy                           0.99      2521\n",
      "   macro avg       0.99      0.99      0.99      2521\n",
      "weighted avg       0.99      0.99      0.99      2521\n",
      "\n",
      "PyTorch Accuracy: 0.990\n"
     ]
    }
   ],
   "source": [
    "model = CombinedTorchModel(X_train_tfidf.shape[1], X_train_num_scaled.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# bundle input and target tensors so we can iterate over it in each epoch\n",
    "train_dataset = TensorDataset(X_text_train_torch, X_num_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # split and shuffle\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    # pass through training set each epoch\n",
    "    for text_batch, num_batch, label_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text_batch, num_batch)\n",
    "        loss = criterion(output, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_text_test_torch, X_num_test_torch)\n",
    "        val_loss = criterion(val_output, y_test_torch)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_probs = model(X_text_test_torch, X_num_test_torch).numpy()\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Apply post-model auto-rejection rules\n",
    "X_test_num_reset = X_test_num.reset_index(drop=True)\n",
    "for i, row in X_test_num_reset.iterrows():\n",
    "    if row['Loan_Amount'] >= 120000 or row['Credit_Score'] < 550 or row['DTI_Ratio'] > 50:\n",
    "        y_pred[i] = 0\n",
    "\n",
    "print(\"\\nPyTorch Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"PyTorch Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7801 - loss: 0.4638 - val_accuracy: 0.9722 - val_loss: 0.1212\n",
      "Epoch 2/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - accuracy: 0.9689 - loss: 0.1075 - val_accuracy: 0.9732 - val_loss: 0.0724\n",
      "Epoch 3/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - accuracy: 0.9800 - loss: 0.0655 - val_accuracy: 0.9881 - val_loss: 0.0465\n",
      "Epoch 4/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - accuracy: 0.9865 - loss: 0.0470 - val_accuracy: 0.9911 - val_loss: 0.0357\n",
      "Epoch 5/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - accuracy: 0.9862 - loss: 0.0454 - val_accuracy: 0.9911 - val_loss: 0.0293\n",
      "Epoch 6/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - accuracy: 0.9906 - loss: 0.0325 - val_accuracy: 0.9970 - val_loss: 0.0219\n",
      "Epoch 7/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - accuracy: 0.9913 - loss: 0.0265 - val_accuracy: 0.9911 - val_loss: 0.0262\n",
      "Epoch 8/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 0.9914 - loss: 0.0276 - val_accuracy: 0.9950 - val_loss: 0.0188\n",
      "Epoch 9/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.9933 - loss: 0.0243 - val_accuracy: 0.9970 - val_loss: 0.0172\n",
      "Epoch 10/10\n",
      "\u001b[1m284/284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 0.9931 - loss: 0.0245 - val_accuracy: 0.9941 - val_loss: 0.0194\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step\n",
      "\n",
      "TensorFlow Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1701\n",
      "           1       0.99      0.99      0.99       820\n",
      "\n",
      "    accuracy                           0.99      2521\n",
      "   macro avg       0.99      0.99      0.99      2521\n",
      "weighted avg       0.99      0.99      0.99      2521\n",
      "\n",
      "TensorFlow Accuracy: 0.993\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433us/step\n",
      "ROC AUC: 1.000\n"
     ]
    }
   ],
   "source": [
    "input_text = Input(shape=(X_train_tfidf.shape[1],))\n",
    "input_num = Input(shape=(X_train_num_scaled.shape[1],))\n",
    "\n",
    "text_branch = Dense(64, activation='sigmoid')(input_text)\n",
    "text_branch = Dense(32, activation='sigmoid')(text_branch) # add another layer\n",
    "\n",
    "num_branch = Dense(32, activation='relu')(input_num)\n",
    "num_branch = Dense(16, activation='relu')(num_branch) # add another layer\n",
    "\n",
    "combined = Concatenate()([text_branch, num_branch])\n",
    "combined = Dense(16, activation='sigmoid')(combined) # add another layer\n",
    "output = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "model_tf = Model(inputs=[input_text, input_num], outputs=output)\n",
    "model_tf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# add validation split to see if we're overfitting\n",
    "model_tf.fit([X_train_tfidf, X_train_num_scaled], y_train, validation_split=0.1, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred_tf = (model_tf.predict([X_test_tfidf, X_test_num_scaled]) > 0.5).astype(int)\n",
    "\n",
    "# Apply post-model auto-rejection rules\n",
    "X_test_num_reset = X_test_num.reset_index(drop=True)\n",
    "for i, row in X_test_num_reset.iterrows():\n",
    "    if row['Loan_Amount'] >= 120000 or row['Credit_Score'] < 550 or row['DTI_Ratio'] > 50:\n",
    "        y_pred[i] = 0\n",
    "\n",
    "print(\"\\nTensorFlow Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tf))\n",
    "\n",
    "accuracy_tf = accuracy_score(y_test, y_pred_tf)\n",
    "print(f\"TensorFlow Accuracy: {accuracy_tf:.3f}\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc = roc_auc_score(y_test, model_tf.predict([X_test_tfidf, X_test_num_scaled]))\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-approval-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
