{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/lukemcguinness/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sklearn as sk\n",
    "import torch as tt\n",
    "import torchvision as ttv\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/853917058.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Approval'].replace(['Approved', 'Rejected'], [1,0], inplace=True)\n",
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/853917058.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Approval'].replace(['Approved', 'Rejected'], [1,0], inplace=True)\n",
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/853917058.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Employment_Status'].replace(['employed', 'unemployed'], [1,0], inplace=True)\n",
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/853917058.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Employment_Status'].replace(['employed', 'unemployed'], [1,0], inplace=True)\n",
      "/var/folders/sk/57wc0kxd0bld0mny_6rdm1g80000gn/T/ipykernel_24656/853917058.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['Text'].apply(lambda x: finalpreprocess(x))\n"
     ]
    }
   ],
   "source": [
    "# apply same logic as in analysis\n",
    "\n",
    "df = pd.read_csv(\"loan_data.csv\")\n",
    "df['Approval'].replace(['Approved', 'Rejected'], [1,0], inplace=True)\n",
    "df['Employment_Status'].replace(['employed', 'unemployed'], [1,0], inplace=True)\n",
    "\n",
    "data = df[(df['DTI_Ratio'] >= 0) & \n",
    "            (df['Credit_Score'] >= 550) & \n",
    "            (df['Loan_Amount'] < 120000)]\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text = text.strip()\n",
    "    text = re.compile(r'[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    return text\n",
    "\n",
    "def stopword(string):\n",
    "    a = [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) \n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for tag in word_pos_tags] \n",
    "    return \" \".join(a)\n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "\n",
    "data['clean_text'] = data['Text'].apply(lambda x: finalpreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = data['clean_text']\n",
    "y = data['Approval']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# convert to numbers and only take 500 most impportant words ** possibly increase/decrease **\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray() # array to be handles by tensor/torch\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray() # array to be handles by tensor/torch\n",
    "\n",
    "# Numeric features\n",
    "numeric_cols = ['Income', 'Credit_Score', 'Loan_Amount', 'DTI_Ratio', 'Employment_Status']\n",
    "X_train_num = data.loc[X_train.index][numeric_cols]\n",
    "X_test_num = data.loc[X_test.index][numeric_cols]\n",
    "\n",
    "# 0 mean and unit variance for input into NN\n",
    "scaler = StandardScaler()\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "X_test_num_scaled = scaler.transform(X_test_num)\n",
    "\n",
    "scaler_reduced = StandardScaler()\n",
    "X_train_num_scaled_reduced = scaler_reduced.fit_transform(X_train_num.drop(['Income'], axis=1))\n",
    "X_num_test_scaled_reduced = scaler_reduced.transform(X_test_num.drop(['Income'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_num_scaled_reduced = scaler.fit_transform(X_train_num.drop(['Loan_Amount'], axis=1))# \n",
    "# ^^ use to test without loan_amount ^^\n",
    "\n",
    "class CombinedTorchModel(nn.Module):\n",
    "    # initialize 3 layers\n",
    "    def __init__(self, text_dim, num_dim):\n",
    "        super().__init__()\n",
    "        self.text_layer = nn.Linear(text_dim, 64)\n",
    "        self.num_layer = nn.Linear(num_dim, 32)\n",
    "        self.combined_layer = nn.Linear(96, 1)\n",
    "\n",
    "    # pass into model\n",
    "    def forward(self, text_input, num_input):\n",
    "        x_text = torch.relu(self.text_layer(text_input))\n",
    "        x_num = torch.relu(self.num_layer(num_input))\n",
    "        x = torch.cat((x_text, x_num), dim=1) # concatenates the outputs from the two branches\n",
    "        return torch.sigmoid(self.combined_layer(x)) # output between 0 and 1\n",
    "    \n",
    "X_text_train_torch = torch.tensor(X_train_tfidf, dtype=torch.float32)\n",
    "X_num_train_torch = torch.tensor(X_train_num_scaled_reduced, dtype=torch.float32)\n",
    "# use unsqueeze(1) to match what model expects\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1) \n",
    "\n",
    "X_text_test_torch = torch.tensor(X_test_tfidf, dtype=torch.float32)\n",
    "X_num_test_torch = torch.tensor(X_num_test_scaled_reduced, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6598\n",
      "Epoch 2, Loss: 0.6557\n",
      "Epoch 3, Loss: 0.6518\n",
      "Epoch 4, Loss: 0.6479\n",
      "Epoch 5, Loss: 0.6440\n",
      "Epoch 6, Loss: 0.6403\n",
      "Epoch 7, Loss: 0.6365\n",
      "Epoch 8, Loss: 0.6328\n",
      "Epoch 9, Loss: 0.6292\n",
      "Epoch 10, Loss: 0.6256\n",
      "\n",
      "PyTorch Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.81      1701\n",
      "           1       0.00      0.00      0.00       820\n",
      "\n",
      "    accuracy                           0.67      2521\n",
      "   macro avg       0.34      0.50      0.40      2521\n",
      "weighted avg       0.46      0.67      0.54      2521\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukemcguinness/.pyenv/versions/3.11.9/envs/loan-approval-env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lukemcguinness/.pyenv/versions/3.11.9/envs/loan-approval-env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lukemcguinness/.pyenv/versions/3.11.9/envs/loan-approval-env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model = CombinedTorchModel(X_train_tfidf.shape[1], X_train_num_scaled_reduced.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_text_train_torch, X_num_train_torch)\n",
    "    loss = criterion(output, y_train_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_probs = model(X_text_test_torch, X_num_test_torch).numpy()\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Apply post-model auto-rejection rules\n",
    "X_test_num_reset = X_test_num.reset_index(drop=True)\n",
    "for i, row in X_test_num_reset.iterrows():\n",
    "    if row['Loan_Amount'] >= 120000 or row['Credit_Score'] < 550 or row['DTI_Ratio'] > 50:\n",
    "        y_pred[i] = 0\n",
    "\n",
    "print(\"\\nPyTorch Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - accuracy: 0.7298 - loss: 0.5147\n",
      "Epoch 2/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - accuracy: 0.9491 - loss: 0.1715\n",
      "Epoch 3/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.9612 - loss: 0.1167\n",
      "Epoch 4/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - accuracy: 0.9689 - loss: 0.0946\n",
      "Epoch 5/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - accuracy: 0.9761 - loss: 0.0803\n",
      "Epoch 6/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.9789 - loss: 0.0707\n",
      "Epoch 7/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491us/step - accuracy: 0.9816 - loss: 0.0690\n",
      "Epoch 8/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.9827 - loss: 0.0594\n",
      "Epoch 9/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.9869 - loss: 0.0556\n",
      "Epoch 10/10\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492us/step - accuracy: 0.9882 - loss: 0.0536\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Apply post-model auto-rejection rules\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X_test_num):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLoan_Amount\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m >= \u001b[32m120000\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m row[\u001b[33m'\u001b[39m\u001b[33mCredit_Score\u001b[39m\u001b[33m'\u001b[39m] < \u001b[32m550\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m row[\u001b[33m'\u001b[39m\u001b[33mDTI_Ratio\u001b[39m\u001b[33m'\u001b[39m] > \u001b[32m50\u001b[39m:\n\u001b[32m     17\u001b[39m         y_pred_tf[i] = \u001b[32m0\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTensorFlow Classification Report:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "input_text = Input(shape=(X_train_tfidf.shape[1],))\n",
    "input_num = Input(shape=(X_train_num_scaled.shape[1],))\n",
    "text_branch = Dense(64, activation='relu')(input_text)\n",
    "num_branch = Dense(32, activation='relu')(input_num)\n",
    "combined = Concatenate()([text_branch, num_branch])\n",
    "output = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "model_tf = Model(inputs=[input_text, input_num], outputs=output)\n",
    "model_tf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_tf.fit([X_train_tfidf, X_train_num_scaled], y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred_tf = (model_tf.predict([X_test_tfidf, X_test_num_scaled]) > 0.5).astype(int)\n",
    "\n",
    "# Apply post-model auto-rejection rules\n",
    "for i, row in enumerate(X_test_num):\n",
    "    if row['Loan_Amount'] >= 120000 or row['Credit_Score'] < 550 or row['DTI_Ratio'] > 50:\n",
    "        y_pred_tf[i] = 0\n",
    "\n",
    "print(\"\\nTensorFlow Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-approval-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
